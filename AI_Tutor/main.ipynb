{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0856e8b9-fa7c-407e-91ef-eb9c3cad3cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: SpeechRecognition in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (3.14.3)\n",
      "Requirement already satisfied: gTTS in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.5.4)\n",
      "Requirement already satisfied: pyttsx3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.99)\n",
      "Requirement already satisfied: transformers in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: typing-extensions in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from SpeechRecognition) (4.15.0)\n",
      "Requirement already satisfied: standard-aifc in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from SpeechRecognition) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from SpeechRecognition) (0.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from gTTS) (2.32.5)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from gTTS) (8.1.8)\n",
      "Requirement already satisfied: colorama in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from click<8.2,>=7.1->gTTS) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gTTS) (2025.8.3)\n",
      "Requirement already satisfied: comtypes in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (1.4.12)\n",
      "Requirement already satisfied: pypiwin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (311)\n",
      "Requirement already satisfied: filelock in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from standard-aifc->SpeechRecognition) (3.13.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install SpeechRecognition gTTS pyttsx3 transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d3525b5-0685-49eb-bf8b-2c069f2a2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def listen():\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"ðŸŽ™ Speak now...\")\n",
    "        r.adjust_for_ambient_noise(source)\n",
    "        audio = r.listen(source)\n",
    "    try:\n",
    "        text = r.recognize_google(audio)\n",
    "        print(\"ðŸ—£ You said:\", text)\n",
    "        return text\n",
    "    except:\n",
    "        print(\"âŒ Sorry, could not recognize speech.\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd4bacfb-8c29-4412-baa8-ac4b87f0d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "qa_model = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "\n",
    "def answer_question(question):\n",
    "    context = \"\"\"\n",
    "    Artificial Intelligence Tutors are virtual systems that help students learn using natural\n",
    "    language processing, voice interaction, and adaptive learning. They make education\n",
    "    accessible for visually impaired users.\n",
    "    \"\"\"\n",
    "    result = qa_model({\n",
    "        \"context\": context,\n",
    "        \"question\": question\n",
    "    })\n",
    "    return result['answer']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d94535e4-c839-4c65-8035-e5a5abfb01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "def speak(text):\n",
    "    tts = gTTS(text=text, lang='en')\n",
    "    tts.save(\"response.mp3\")\n",
    "    os.system(\"start response.mp3\")  # For Windows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d7ff097-eb05-4920-85a2-22674e0132f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŽ™ Speak now...\n",
      "ðŸ—£ You said: hello explain the concept of OP\n",
      "ðŸ’¡ AI Tutor: They make education\n",
      "    accessible for visually impaired users\n",
      "ðŸŽ™ Speak now...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      2\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m         query = \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m query.lower() \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mexit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mquit\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m      5\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸ‘‹ Goodbye!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mlisten\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mðŸŽ™ Speak now...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m     r.adjust_for_ambient_noise(source)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     audio = \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m     text = r.recognize_google(audio)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[39m, in \u001b[36mRecognizer.listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    458\u001b[39m result = \u001b[38;5;28mself\u001b[39m._listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:492\u001b[39m, in \u001b[36mRecognizer._listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time > timeout:\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[33m\"\u001b[39m\u001b[33mlistening timed out while waiting for phrase to start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m buffer = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[32m    494\u001b[39m frames.append(buffer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[39m, in \u001b[36mMicrophone.MicrophoneStream.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpyaudio_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[39m, in \u001b[36mPyAudio.Stream.read\u001b[39m\u001b[34m(self, num_frames, exception_on_overflow)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_input:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot input stream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    569\u001b[39m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    while True:\n",
    "        query = listen()\n",
    "        if query.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
    "            print(\"ðŸ‘‹ Goodbye!\")\n",
    "            break\n",
    "        response = answer_question(query)\n",
    "        print(\"ðŸ’¡ AI Tutor:\", response)\n",
    "        speak(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e006f2-3450-41e9-a6bb-b4d99f80a4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_tutor_env)",
   "language": "python",
   "name": "ai_tutor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
