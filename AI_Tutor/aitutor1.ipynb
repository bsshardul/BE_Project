{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc783ec6-7577-4dc8-9a86-94c67c386694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechrecognition in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (3.14.3)\n",
      "Requirement already satisfied: gtts in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.5.4)\n",
      "Requirement already satisfied: pyttsx3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.99)\n",
      "Requirement already satisfied: transformers in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.3.2)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: typing-extensions in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (4.15.0)\n",
      "Requirement already satisfied: standard-aifc in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (0.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from gtts) (2.32.5)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from gtts) (8.1.8)\n",
      "Requirement already satisfied: colorama in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests<3,>=2.27->gtts) (2025.8.3)\n",
      "Requirement already satisfied: comtypes in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (1.4.12)\n",
      "Requirement already satisfied: pypiwin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (311)\n",
      "Requirement already satisfied: filelock in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: standard-chunk in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from standard-aifc->speechrecognition) (3.13.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "\n",
      "   ---------------------------------------- 0/2 [et-xmlfile]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   -------------------- ------------------- 1/2 [openpyxl]\n",
      "   ---------------------------------------- 2/2 [openpyxl]\n",
      "\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip install speechrecognition gtts pyttsx3 transformers torch pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be63f125-9d0d-45bf-a38b-95f4e4b91725",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "# Optional: import for RAG if you start building it later\n",
    "# from sentence_transformers import SentenceTransformer \n",
    "# import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e2d998c-eac7-4518-8572-a56acaa01fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize Model and Global Variables\n",
    "try:\n",
    "    QA_MODEL = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR: Could not load QA model. {e}\")\n",
    "    QA_MODEL = None\n",
    "\n",
    "KNOWLEDGE_BASE_DF = None\n",
    "LAST_RESPONSE_TEXT = \"\"\n",
    "\n",
    "def load_knowledge_base(file_path=\"tutor_knowledge.xlsx\"):\n",
    "    \"\"\"Loads the Excel sheet into a pandas DataFrame.\"\"\"\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "    try:\n",
    "        KNOWLEDGE_BASE_DF = pd.read_excel(file_path, header=0)\n",
    "        print(f\"‚úÖ Loaded {len(KNOWLEDGE_BASE_DF)} entries from {file_path}.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Excel file not found at {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR loading Excel: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "feaa2faf-52f0-4cd9-98fc-b2554fc73a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. VOICE INPUT MODULE ---\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Captures audio from the microphone and converts it to text.\"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"\\n\\nüéôÔ∏è Listening...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.5)\n",
    "        try:\n",
    "            audio = r.listen(source, timeout=5, phrase_time_limit=10)\n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"‚ùå No speech detected within time limit.\")\n",
    "            return \"\"\n",
    "\n",
    "    try:\n",
    "        # Use Google Speech Recognition for transcription (requires internet)\n",
    "        text = r.recognize_google(audio)\n",
    "        print(f\"üó£Ô∏è You said: {text}\")\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"‚ùå Sorry, could not recognize speech.\")\n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        print(\"‚ùå Could not request results from Google Speech Recognition service; check internet.\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fce9bf7d-656e-4f43-8086-63f37ca9e7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def braille_convert_and_print(last_text_response):\n",
    "    \"\"\"\n",
    "    Finds the associated Braille output from the DF and simulates printing.\n",
    "    \"\"\"\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return speak(\"Knowledge base not loaded to find Braille content.\")\n",
    "        \n",
    "    # 1. Look up the key term in the DataFrame\n",
    "    # Find rows where the generated response matches the last spoken text\n",
    "    match = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['Generated Audio Response (Text for TTS)'] == last_text_response\n",
    "    ]\n",
    "    \n",
    "    if not match.empty and match.iloc[0]['Braille Conversion Required (Yes/No)'].lower() == 'yes':\n",
    "        key_term = match.iloc[0]['Braille Output (Key Term/Formula/Code)']\n",
    "        \n",
    "        # --- Actual Braille Simulation ---\n",
    "        print(\"-\" * 50)\n",
    "        print(\"üñ®Ô∏è TACTILE OUTPUT INITIATED üñ®Ô∏è\")\n",
    "        print(f\"   Key Term for Braille: {key_term}\")\n",
    "        # NOTE: For real implementation, replace with pybrl/pyserial logic\n",
    "        # For now, we simulate success:\n",
    "        print(f\"   Simulated Braille Embossing for: {key_term}\")\n",
    "        print(\"-\" * 50)\n",
    "        speak(f\"The key information: {key_term} has been sent to the Braille embosser.\")\n",
    "        \n",
    "    else:\n",
    "        speak(\"I found no matching Braille content for the last answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78f8c132-0345-420d-b6e7-096e48a151c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. TEXT-TO-SPEECH (TTS) MODULE ---\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Converts text to speech using gTTS, saves it, and plays it.\"\"\"\n",
    "    global LAST_RESPONSE_TEXT\n",
    "    \n",
    "    if not text:\n",
    "        return\n",
    "        \n",
    "    LAST_RESPONSE_TEXT = text # Store the response for command handling\n",
    "    \n",
    "    try:\n",
    "        # 1. Convert\n",
    "        tts = gTTS(text=text, lang='en')\n",
    "        \n",
    "        # 2. Save (use a timestamp for a unique file name)\n",
    "        filename = f\"response_{int(time.time())}.mp3\"\n",
    "        tts.save(filename)\n",
    "        \n",
    "        # 3. Play (using standard Windows 'start' command)\n",
    "        # NOTE: If on Mac/Linux, replace 'start' with 'afplay' or 'mpg123'\n",
    "        print(f\"üí° AI Tutor: {text}\")\n",
    "        os.system(f\"start {filename}\") \n",
    "\n",
    "        # 4. Optional: Clean up the audio file after a short pause\n",
    "        # time.sleep(len(text) / 10 + 2) # Wait proportional to length\n",
    "        # os.remove(filename)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during text-to-speech: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f9dd893-6673-458f-83ea-9c600679e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    1. Simulates retrieval by finding the closest entry in the DataFrame based on keyword/intent.\n",
    "    2. Uses the retrieved context for the QA model to extract the final answer.\n",
    "    \"\"\"\n",
    "    # Ensure the global QA_MODEL and KNOWLEDGE_BASE_DF are available\n",
    "    global QA_MODEL \n",
    "    global KNOWLEDGE_BASE_DF\n",
    "\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return \"Knowledge base not ready.\"\n",
    "        \n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # 1. RETRIEVAL LOGIC (Searching the DataFrame)\n",
    "    \n",
    "    # Check the 'NLP Intent / Target Topic' column first for broader matching\n",
    "    matching_rows = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['NLP Intent / Target Topic'].str.lower().str.contains(question_lower, na=False)\n",
    "    ]\n",
    "    \n",
    "    # Check the 'User Voice Input' column as a secondary check if no intent match is found\n",
    "    if matching_rows.empty:\n",
    "        matching_rows = KNOWLEDGE_BASE_DF[\n",
    "            KNOWLEDGE_BASE_DF['User Voice Input (Engineering Topic)'].str.lower().str.contains(question_lower, na=False)\n",
    "        ]\n",
    "        \n",
    "    if not matching_rows.empty:\n",
    "        # Use the context associated with the best match (first one found)\n",
    "        context = matching_rows.iloc[0]['Context/Source (Pre-stored or AI-Gen)']\n",
    "        print(f\"   [Retrieved Context from Excel for: {matching_rows.iloc[0]['Query ID']}]\")\n",
    "    else:\n",
    "        # FALLBACK: Combine only the first 5 contexts to avoid truncation issues\n",
    "        print(\"   [Using combined context fallback for general query]\")\n",
    "        # .head(5) is critical here to limit the context length\n",
    "        context = \" \".join(KNOWLEDGE_BASE_DF['Context/Source (Pre-stored or AI-Gen)'].astype(str).head(5).tolist())\n",
    "\n",
    "    # 2. QUESTION ANSWERING (Calling the NLP Model)\n",
    "    \n",
    "    # Pass the retrieved context and the user's question to the QA model\n",
    "    try:\n",
    "        if QA_MODEL is None:\n",
    "            return \"QA model is not initialized.\"\n",
    "            \n",
    "        result = QA_MODEL({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # Return the extracted answer\n",
    "        return result['answer']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during QA processing: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2ca59f3-4f71-4be6-836e-0c10508d11d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 100 entries from tutor_knowledge.xlsx.\n",
      "ü§ñ AI Tutor is ready. Speak your engineering question or say 'quit'.\n",
      "üí° AI Tutor: Hello! I am your AI engineering tutor. How can I help you learn today?\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "‚ùå Sorry, could not recognize speech.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "üó£Ô∏è You said: what is Doppler\n",
      "   [Using combined context fallback for general query]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° AI Tutor: System Help\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "‚ùå Sorry, could not recognize speech.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "üó£Ô∏è You said: what is ideal gas\n",
      "   [Using combined context fallback for general query]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° AI Tutor: Electronics Module\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "‚ùå Sorry, could not recognize speech.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "‚ùå Sorry, could not recognize speech.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "üó£Ô∏è You said: word for and\n",
      "   [Using combined context fallback for general query]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° AI Tutor: System Help\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "üó£Ô∏è You said: what are the steps of water cycle\n",
      "   [Using combined context fallback for general query]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\transformers\\pipelines\\question_answering.py:395: FutureWarning: Passing a list of SQuAD examples to the pipeline is deprecated and will be removed in v5. Inputs should be passed using the `question` and `context` keyword arguments instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí° AI Tutor: Math Module AI-Generated Summary Pre-stored: System Help\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "‚ùå Sorry, could not recognize speech.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "üó£Ô∏è You said: quit\n",
      "üí° AI Tutor: Goodbye! Happy studying.\n",
      "üëã Session ended.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    if not load_knowledge_base(): \n",
    "        print(\"\\nFATAL ERROR: Cannot proceed without the knowledge base. Check file name/path.\")\n",
    "    else:\n",
    "        # 2. Start Session\n",
    "        print(\"ü§ñ AI Tutor is ready. Speak your engineering question or say 'quit'.\")\n",
    "        speak(\"Hello! I am your AI engineering tutor. How can I help you learn today?\")\n",
    "        \n",
    "        while True:\n",
    "            query = listen()\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            # 3. Command Handling\n",
    "            if query_lower in [\"exit\", \"quit\", \"stop\", \"i am done\"]:\n",
    "                speak(\"Goodbye! Happy studying.\")\n",
    "                print(\"üëã Session ended.\")\n",
    "                break\n",
    "            \n",
    "            elif \"read again\" in query_lower or \"say that again\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    speak(LAST_RESPONSE_TEXT)\n",
    "                else:\n",
    "                    speak(\"I don't have a previous response to repeat.\")\n",
    "                continue\n",
    "                \n",
    "            elif \"braille\" in query_lower or \"tactile\" in query_lower or \"print\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    braille_convert_and_print(LAST_RESPONSE_TEXT)\n",
    "                else:\n",
    "                    speak(\"I need to answer a question before I can convert a response to Braille.\")\n",
    "                continue\n",
    "\n",
    "            # 4. Question Answering\n",
    "            response = answer_question(query)\n",
    "            speak(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e4ed923-5694-4add-ad1b-011b98aec212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QA Model (DistilBERT) initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 72.1M/72.1M [00:03<00:00, 20.6MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Whisper Model (Tiny, CPU Optimized) loaded.\n",
      "‚úÖ Loaded 100 entries from tutor_knowledge.xlsx.\n",
      "\n",
      "==================================================\n",
      "ü§ñ AI Tutor is ready. Speak your engineering question.\n",
      "==================================================\n",
      "üí° AI Tutor: Hello! I am your AI engineering tutor. How can I help you learn today?\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Let's go.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Explain Ohm's Law.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: The speaker is discussing engineering topics, C++ syntax, C++ syntax, C++ syntax, C++ syntax,\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Quick...\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Exit.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 271\u001b[39m\n\u001b[32m    268\u001b[39m speak(\u001b[33m\"\u001b[39m\u001b[33mHello! I am your AI engineering tutor. How can I help you learn today?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     query = \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query:\n\u001b[32m    274\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 75\u001b[39m, in \u001b[36mlisten\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m sr.Microphone() \u001b[38;5;28;01mas\u001b[39;00m source:\n\u001b[32m     74\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müéôÔ∏è Listening...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43madjust_for_ambient_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mduration\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     77\u001b[39m         audio = r.listen(source, timeout=\u001b[32m8\u001b[39m, phrase_time_limit=\u001b[32m15\u001b[39m) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:385\u001b[39m, in \u001b[36mRecognizer.adjust_for_ambient_noise\u001b[39m\u001b[34m(self, source, duration)\u001b[39m\n\u001b[32m    383\u001b[39m elapsed_time += seconds_per_buffer\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m elapsed_time > duration: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m buffer = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m energy = audioop.rms(buffer, source.SAMPLE_WIDTH)  \u001b[38;5;66;03m# energy of the audio signal\u001b[39;00m\n\u001b[32m    388\u001b[39m \u001b[38;5;66;03m# dynamically adjust the energy threshold using asymmetric weighted average\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[39m, in \u001b[36mMicrophone.MicrophoneStream.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpyaudio_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[39m, in \u001b[36mPyAudio.Stream.read\u001b[39m\u001b[34m(self, num_frames, exception_on_overflow)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_input:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot input stream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    569\u001b[39m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import whisper # Added for local STT\n",
    "import numpy as np # Added for robust handling\n",
    "\n",
    "# --- GLOBAL CONFIGURATION AND KNOWLEDGE ---\n",
    "# List of technical keywords to guide the RAG system to the correct module.\n",
    "TECHNICAL_KEYWORDS = {\n",
    "    'ohm': 'NLP Intent / Target Topic',\n",
    "    'voltage': 'NLP Intent / Target Topic',\n",
    "    'pn junction': 'NLP Intent / Target Topic',\n",
    "    'semiconductor': 'NLP Intent / Target Topic',\n",
    "    'linear equation': 'NLP Intent / Target Topic',\n",
    "    'matrix': 'NLP Intent / Target Topic',\n",
    "    'transformer': 'NLP Intent / Target Topic',\n",
    "    'logic gate': 'NLP Intent / Target Topic',\n",
    "    'and or': 'NLP Intent / Target Topic',\n",
    "    'braille': 'NLP Intent / Target Topic',\n",
    "    'replay': 'NLP Intent / Target Topic',\n",
    "    'last explanation': 'NLP Intent / Target Topic',\n",
    "    'if statement': 'NLP Intent / Target Topic',\n",
    "}\n",
    "\n",
    "# --- INITIALIZATION ---\n",
    "# 1. QA Model (RAG Generation)\n",
    "try:\n",
    "    # DistilBERT is already small and CPU-friendly, keeping it as the RAG generator.\n",
    "    QA_MODEL = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "    print(\"‚úÖ QA Model (DistilBERT) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load QA model. {e}\")\n",
    "    QA_MODEL = None\n",
    "\n",
    "# 2. STT Model (Local, CPU Optimized)\n",
    "try:\n",
    "    # Using the 'tiny' model for maximum speed and minimum RAM/CPU requirements.\n",
    "    WHISPER_MODEL = whisper.load_model(\"tiny\") \n",
    "    \n",
    "    # Context prompt to guide transcription towards engineering vocabulary\n",
    "    WHISPER_PROMPT = \"The speaker is discussing engineering topics, P-N junction, Ohm's Law, linear equations, C++ syntax, logic gates, transformers, voltage, and current.\"\n",
    "    print(\"‚úÖ Whisper Model (Tiny, CPU Optimized) loaded.\")\n",
    "except Exception as e:\n",
    "    # This error often occurs if Pytorch or FFmpeg is missing.\n",
    "    print(f\"‚ùå ERROR: Could not load Whisper model. Switching to Google STT fallback. {e}\")\n",
    "    WHISPER_MODEL = None\n",
    "\n",
    "KNOWLEDGE_BASE_DF = None\n",
    "LAST_RESPONSE_TEXT = \"\"\n",
    "\n",
    "def load_knowledge_base(file_path=\"tutor_knowledge.xlsx\"):\n",
    "    \"\"\"Loads the Excel sheet into a pandas DataFrame.\"\"\"\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "    try:\n",
    "        KNOWLEDGE_BASE_DF = pd.read_excel(file_path, header=0)\n",
    "        print(f\"‚úÖ Loaded {len(KNOWLEDGE_BASE_DF)} entries from {file_path}.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Excel file not found at {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR loading Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- 2. VOICE INPUT MODULE (MODIFIED FOR LOCAL WHISPER) ---\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Captures audio and converts it to text using local Whisper (CPU preferred) or Google STT.\"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"\\n\\nüéôÔ∏è Listening...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.5)\n",
    "        try:\n",
    "            audio = r.listen(source, timeout=8, phrase_time_limit=15) \n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"‚ùå No speech detected within time limit.\")\n",
    "            return \"\"\n",
    "\n",
    "    try:\n",
    "        # --- LOCAL WHISPER STT (CPU PRIORITY) ---\n",
    "        if WHISPER_MODEL:\n",
    "            # Save audio temporarily for Whisper to read\n",
    "            temp_wav_file = \"temp_audio_stt.wav\"\n",
    "            with open(temp_wav_file, \"wb\") as f:\n",
    "                f.write(audio.get_wav_data())\n",
    "\n",
    "            # Use Whisper for local, context-aware transcription\n",
    "            print(\"  [STT] Processing locally with Whisper (Tiny)...\")\n",
    "            result = WHISPER_MODEL.transcribe(\n",
    "                temp_wav_file, \n",
    "                initial_prompt=WHISPER_PROMPT, \n",
    "                language='en',\n",
    "                # Beam size 3 is faster than default 5 on CPU\n",
    "                beam_size=3\n",
    "            )\n",
    "            text = result[\"text\"].strip()\n",
    "            os.remove(temp_wav_file) # Clean up\n",
    "        \n",
    "        else:\n",
    "            # --- GOOGLE STT FALLBACK (Internet Required) ---\n",
    "            print(\"  [STT] Falling back to Google STT (Internet Required)...\")\n",
    "            text = r.recognize_google(audio)\n",
    "            \n",
    "        print(f\"üó£Ô∏è You said: {text}\")\n",
    "        return text\n",
    "        \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"‚ùå Sorry, could not recognize speech. (Tip: Try speaking slower, or rephrase technical terms.)\") \n",
    "        return \"\"\n",
    "    except sr.RequestError:\n",
    "        print(\"‚ùå Could not request results from Google Speech Recognition service; check internet.\")\n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå STT Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- 3. TEXT-TO-SPEECH (TTS) MODULE ---\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Converts text to speech using gTTS, saves it, and plays it.\"\"\"\n",
    "    global LAST_RESPONSE_TEXT\n",
    "    \n",
    "    if not text:\n",
    "        return\n",
    "        \n",
    "    LAST_RESPONSE_TEXT = text # Store the response for command handling\n",
    "    \n",
    "    try:\n",
    "        tts = gTTS(text=text, lang='en')\n",
    "        filename = f\"response_{int(time.time())}.mp3\"\n",
    "        tts.save(filename)\n",
    "        \n",
    "        # NOTE: The print statement now includes the source/context ID for observability!\n",
    "        print(f\"üí° AI Tutor: {text}\")\n",
    "        os.system(f\"start {filename}\")  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during text-to-speech: {e}\")\n",
    "\n",
    "# --- 4. BRAILLE MODULE ---\n",
    "\n",
    "def braille_convert_and_print(last_text_response):\n",
    "    \"\"\"\n",
    "    Finds the associated Braille output from the DF and simulates printing.\n",
    "    \"\"\"\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return speak(\"Knowledge base not loaded to find Braille content.\")\n",
    "        \n",
    "    # Search the DF for the row that generated the last audio response\n",
    "    # We strip the source ID \"(E00X)\" from the start of the response text before lookup\n",
    "    clean_response = last_text_response.split(') ', 1)[-1]\n",
    "    \n",
    "    match = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['Generated Audio Response (Text for TTS)'] == clean_response\n",
    "    ]\n",
    "    \n",
    "    # Use .iloc[0] for safety and .any() for the boolean Series result\n",
    "    if not match.empty and match['Braille Conversion Required (Yes/No)'].str.lower().str.contains('yes', na=False).any():\n",
    "        key_term = match.iloc[0]['Braille Output (Key Term/Formula/Code)']\n",
    "        \n",
    "        # --- Actual Braille Simulation ---\n",
    "        print(\"-\" * 50)\n",
    "        print(\"üñ®Ô∏è TACTILE OUTPUT INITIATED üñ®Ô∏è\")\n",
    "        print(f\"    Key Term for Braille: {key_term}\")\n",
    "        # NOTE: Using the standard LaTeX notation for clean output (e.g., V=IR)\n",
    "        print(\"-\" * 50)\n",
    "        speak(f\"The key information: {key_term} has been sent to the Braille embosser.\")\n",
    "        \n",
    "    else:\n",
    "        speak(\"I found no matching Braille content for the last answer.\")\n",
    "\n",
    "# --- 5. RAG CORE MODULE ---\n",
    "\n",
    "def find_best_context(question):\n",
    "    \"\"\"\n",
    "    NEW TARGETED RETRIEVAL: Uses keyword mapping for high-confidence RAG retrieval.\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # 1. Check for direct keyword matches using the predefined TECHNICAL_KEYWORDS dictionary\n",
    "    for term, target_col in TECHNICAL_KEYWORDS.items():\n",
    "        # Check if the technical term is present in the user's question\n",
    "        if term in question_lower:\n",
    "            # Look for the row where the keyword appears in the designated target column\n",
    "            # Use regex=True for robust matching\n",
    "            matching_rows = KNOWLEDGE_BASE_DF[\n",
    "                KNOWLEDGE_BASE_DF[target_col].astype(str).str.lower().str.contains(term, regex=True, na=False)\n",
    "            ]\n",
    "            \n",
    "            if not matching_rows.empty:\n",
    "                # SUCCESS: Return the context of the highest matching row (first one found)\n",
    "                row = matching_rows.iloc[0]\n",
    "                context = row['Context/Source (Pre-stored or AI-Gen)']\n",
    "                source_id = row['Query ID']\n",
    "                \n",
    "                print(f\"  [RAG STEP 1] Targeted Retrieval Success using '{term}'. Document ID: {source_id}\")\n",
    "                return context, source_id\n",
    "                \n",
    "    # 2. FALLBACK: Search the primary content column for the full question\n",
    "    print(\"  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\")\n",
    "    \n",
    "    # Search the 'User Voice Input' column for the complete, transcribed user query\n",
    "    matching_rows_fallback = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['User Voice Input (Engineering Topic)'].astype(str).str.lower().str.contains(question_lower, regex=True, na=False)\n",
    "    ]\n",
    "    \n",
    "    if not matching_rows_fallback.empty:\n",
    "        row = matching_rows_fallback.iloc[0]\n",
    "        context = row['Context/Source (Pre-stored or AI-Gen)']\n",
    "        source_id = row['Query ID']\n",
    "        \n",
    "        print(f\"  [RAG STEP 1] Fallback Success. Context found by matching full query: {source_id}\")\n",
    "        return context, source_id\n",
    "        \n",
    "    # 3. FAILURE: If RAG fails entirely.\n",
    "    return None, None \n",
    "\n",
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    1. Retrieves the most relevant context using the new keyword logic.\n",
    "    2. Uses the retrieved context for the QA model to extract the final answer.\n",
    "    \"\"\"\n",
    "    global QA_MODEL\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return \"Knowledge base not ready.\"\n",
    "\n",
    "    # --- 1. RETRIEVAL (RAG STEP) ---\n",
    "    context, source_id = find_best_context(question)\n",
    "    \n",
    "    if context is None:\n",
    "        # If retrieval fails, return a knowledge base not found message\n",
    "        return \"I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\"\n",
    "        \n",
    "    # --- 2. GENERATION (QA MODEL) ---\n",
    "    try:\n",
    "        if QA_MODEL is None:\n",
    "            return \"QA model is not initialized.\"\n",
    "            \n",
    "        result = QA_MODEL({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # RAG OBSERVABILITY: Print the source used for the answer\n",
    "        print(f\"  [RAG STEP 2] Answer generated from Source ID: {source_id}\")\n",
    "        \n",
    "        # Append the source to the final answer text for visual proof in the demo\n",
    "        final_answer = f\"({source_id}) {result['answer']}\"\n",
    "        return final_answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during QA processing: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Load Data\n",
    "    if not load_knowledge_base():  \n",
    "        print(\"\\nFATAL ERROR: Cannot proceed without the knowledge base. Check file name/path.\")\n",
    "    else:\n",
    "        # 2. Start Session\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ü§ñ AI Tutor is ready. Speak your engineering question.\")\n",
    "        print(\"=\"*50)\n",
    "        speak(\"Hello! I am your AI engineering tutor. How can I help you learn today?\")\n",
    "        \n",
    "        while True:\n",
    "            query = listen()\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            # 3. Command Handling\n",
    "            if query_lower in [\"exit\", \"quit\", \"stop\", \"i am done\"]:\n",
    "                speak(\"Goodbye! Happy studying.\")\n",
    "                print(\"üëã Session ended.\")\n",
    "                break\n",
    "                \n",
    "            elif \"read again\" in query_lower or \"say that again\" in query_lower or \"replay\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    # NOTE: Replaying last response text without the source ID prefix.\n",
    "                    clean_response = LAST_RESPONSE_TEXT.split(') ', 1)[-1]\n",
    "                    speak(f\"Replaying last response: {clean_response}\")\n",
    "                else:\n",
    "                    speak(\"I don't have a previous response to repeat.\")\n",
    "                continue\n",
    "                \n",
    "            elif \"braille\" in query_lower or \"tactile\" in query_lower or \"print\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    braille_convert_and_print(LAST_RESPONSE_TEXT)\n",
    "                else:\n",
    "                    speak(\"I need to answer a question before I can convert a response to Braille.\")\n",
    "                continue\n",
    "\n",
    "            # 4. Question Answering (RAG Pipeline)\n",
    "            response = answer_question(query)\n",
    "            speak(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3036968c-73cb-47d6-8b46-caaf8c0142db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyttsx3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.99)\n",
      "Requirement already satisfied: comtypes in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (1.4.12)\n",
      "Requirement already satisfied: pypiwin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (311)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22299c63-f2e4-4a1a-8e95-97949f612da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: speechrecognition in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (3.14.3)\n",
      "Requirement already satisfied: pyttsx3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.99)\n",
      "Requirement already satisfied: transformers in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: torch in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: pandas in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (2.3.2)\n",
      "Requirement already satisfied: openpyxl in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (3.1.5)\n",
      "Collecting whisper\n",
      "  Downloading whisper-1.1.10.tar.gz (42 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing-extensions in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (4.15.0)\n",
      "Requirement already satisfied: standard-aifc in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from speechrecognition) (0.2.2)\n",
      "Requirement already satisfied: comtypes in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (1.4.12)\n",
      "Requirement already satisfied: pypiwin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (223)\n",
      "Requirement already satisfied: pywin32 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pyttsx3) (311)\n",
      "Requirement already satisfied: filelock in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: et-xmlfile in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from whisper) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: standard-chunk in h:\\be\\42310_be_project\\.venv\\lib\\site-packages (from standard-aifc->speechrecognition) (3.13.0)\n",
      "Building wheels for collected packages: whisper\n",
      "  Building wheel for whisper (pyproject.toml): started\n",
      "  Building wheel for whisper (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for whisper: filename=whisper-1.1.10-py3-none-any.whl size=41476 sha256=9e081f1db4b3ceaeac1e0400de7e2288f83f7c698fd690bbc1b04007e6fe2d50\n",
      "  Stored in directory: c:\\users\\s bramhanathkar\\appdata\\local\\pip\\cache\\wheels\\7e\\1e\\f0\\d36b92489c74925c5aa1aeb01d30f39ba018d2a1914e79ac36\n",
      "Successfully built whisper\n",
      "Installing collected packages: whisper\n",
      "Successfully installed whisper-1.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install speechrecognition pyttsx3 transformers torch pandas openpyxl whisper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9509c64c-663b-468d-bc14-40c0b1bb5ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ QA Model (DistilBERT) initialized.\n",
      "‚úÖ Whisper Model (Tiny, CPU Optimized) loaded.\n",
      "‚úÖ pyttsx3 (Offline TTS) initialized.\n",
      "‚úÖ Loaded 100 entries from tutor_knowledge.xlsx.\n",
      "\n",
      "==================================================\n",
      "ü§ñ AI Tutor is ready. Speak your engineering question.\n",
      "==================================================\n",
      "üí° AI Tutor: Hello! I am your AI engineering tutor. How can I help you learn today?\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: What's on this Law?\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Explain the N junction.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: The speaker is discussing engineering topics, C++ syntax, C++ syntax, C++ syntax, C++ syntax, C++ syntax, C++ syntax,\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: Explain the N junction.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n",
      "  [STT] Processing locally with Whisper (Tiny)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "H:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\whisper\\transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è You said: The law of thermodynamics.\n",
      "  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\n",
      "üí° AI Tutor: I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\n",
      "\n",
      "\n",
      "üéôÔ∏è Listening...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 301\u001b[39m\n\u001b[32m    298\u001b[39m speak(\u001b[33m\"\u001b[39m\u001b[33mHello! I am your AI engineering tutor. How can I help you learn today?\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    300\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     query = \u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m query:\n\u001b[32m    304\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mlisten\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    101\u001b[39m r.adjust_for_ambient_noise(source, duration=\u001b[32m0.5\u001b[39m)\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m     audio = \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlisten\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphrase_time_limit\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m15\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m sr.WaitTimeoutError:\n\u001b[32m    105\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå No speech detected within time limit.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:460\u001b[39m, in \u001b[36mRecognizer.listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    458\u001b[39m result = \u001b[38;5;28mself\u001b[39m._listen(source, timeout, phrase_time_limit, snowboy_configuration, stream)\n\u001b[32m    459\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n\u001b[32m--> \u001b[39m\u001b[32m460\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    461\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:492\u001b[39m, in \u001b[36mRecognizer._listen\u001b[39m\u001b[34m(self, source, timeout, phrase_time_limit, snowboy_configuration, stream)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mand\u001b[39;00m elapsed_time > timeout:\n\u001b[32m    490\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m WaitTimeoutError(\u001b[33m\"\u001b[39m\u001b[33mlistening timed out while waiting for phrase to start\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m buffer = \u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCHUNK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(buffer) == \u001b[32m0\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# reached end of the stream\u001b[39;00m\n\u001b[32m    494\u001b[39m frames.append(buffer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\speech_recognition\\__init__.py:191\u001b[39m, in \u001b[36mMicrophone.MicrophoneStream.read\u001b[39m\u001b[34m(self, size)\u001b[39m\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[32m--> \u001b[39m\u001b[32m191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpyaudio_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mH:\\BE\\42310_BE_Project\\.venv\\Lib\\site-packages\\pyaudio\\__init__.py:570\u001b[39m, in \u001b[36mPyAudio.Stream.read\u001b[39m\u001b[34m(self, num_frames, exception_on_overflow)\u001b[39m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_input:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNot input stream\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    569\u001b[39m                   paCanNotReadFromAnOutputOnlyStream)\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_stream\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_stream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m                      \u001b[49m\u001b[43mexception_on_overflow\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "# We replace gtts with pyttsx3 for offline TTS\n",
    "import pyttsx3 \n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import whisper\n",
    "import numpy as np \n",
    "import re # Import the regular expression module\n",
    "\n",
    "# --- GLOBAL CONFIGURATION AND KNOWLEDGE ---\n",
    "# List of technical keywords to guide the RAG system to the correct module.\n",
    "TECHNICAL_KEYWORDS = {\n",
    "    # Engineering concepts prioritize searching the 'User Voice Input' column \n",
    "    'ohm': 'User Voice Input (Engineering Topic)',\n",
    "    'voltage': 'User Voice Input (Engineering Topic)',\n",
    "    'formula': 'User Voice Input (Engineering Topic)', \n",
    "    'pn junction': 'User Voice Input (Engineering Topic)',\n",
    "    'semiconductor': 'User Voice Input (Engineering Topic)',\n",
    "    'linear equation': 'User Voice Input (Engineering Topic)',\n",
    "    'matrix': 'User Voice Input (Engineering Topic)',\n",
    "    'transformer': 'User Voice Input (Engineering Topic)',\n",
    "    'logic gate': 'User Voice Input (Engineering Topic)',\n",
    "    'if statement': 'User Voice Input (Engineering Topic)',\n",
    "    \n",
    "    # System commands should look in the column containing the target topic/intent\n",
    "    'braille': 'NLP Intent / Target Topic',\n",
    "    'replay': 'NLP Intent / Target Topic',\n",
    "    'last explanation': 'NLP Intent / Target Topic',\n",
    "}\n",
    "\n",
    "# --- INITIALIZATION ---\n",
    "# 1. QA Model (RAG Generation)\n",
    "try:\n",
    "    QA_MODEL = pipeline(\"question-answering\", model=\"distilbert-base-uncased-distilled-squad\")\n",
    "    print(\"‚úÖ QA Model (DistilBERT) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load QA model. {e}\")\n",
    "    QA_MODEL = None\n",
    "\n",
    "# 2. STT Model (Local, CPU Optimized)\n",
    "try:\n",
    "    WHISPER_MODEL = whisper.load_model(\"tiny\") \n",
    "    WHISPER_PROMPT = \"The speaker is discussing engineering topics, P-N junction, Ohm's Law, linear equations, C++ syntax, logic gates, transformers, voltage, and current.\"\n",
    "    print(\"‚úÖ Whisper Model (Tiny, CPU Optimized) loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not load Whisper model. STT will fail if disconnected from the internet. {e}\")\n",
    "    WHISPER_MODEL = None\n",
    "\n",
    "# 3. TTS Engine (Local, Offline)\n",
    "try:\n",
    "    TTS_ENGINE = pyttsx3.init()\n",
    "    # Optional: Adjust rate for clarity on low-spec systems\n",
    "    TTS_ENGINE.setProperty('rate', 150) # Words per minute\n",
    "    print(\"‚úÖ pyttsx3 (Offline TTS) initialized.\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR: Could not initialize pyttsx3. TTS will not function. {e}\")\n",
    "    TTS_ENGINE = None\n",
    "\n",
    "KNOWLEDGE_BASE_DF = None\n",
    "LAST_RESPONSE_TEXT = \"\"\n",
    "\n",
    "def load_knowledge_base(file_path=\"tutor_knowledge.xlsx\"):\n",
    "    \"\"\"Loads the Excel sheet into a pandas DataFrame.\"\"\"\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "    try:\n",
    "        KNOWLEDGE_BASE_DF = pd.read_excel(file_path, header=0)\n",
    "        print(f\"‚úÖ Loaded {len(KNOWLEDGE_BASE_DF)} entries from {file_path}.\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå ERROR: Excel file not found at {file_path}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR loading Excel: {e}\")\n",
    "        return False\n",
    "\n",
    "def show_data_status():\n",
    "    \"\"\"Prints metadata about the loaded knowledge base for accessibility proof.\"\"\"\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        speak(\"The knowledge base is not currently loaded.\")\n",
    "        return\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "    print(\"--- KNOWLEDGE BASE STATUS ---\")\n",
    "    print(f\"Total Entries (Rows): {len(KNOWLEDGE_BASE_DF)}\")\n",
    "    print(f\"Total Columns: {len(KNOWLEDGE_BASE_DF.columns)}\")\n",
    "    print(f\"Primary Retrieval Column: {'User Voice Input (Engineering Topic)'}\")\n",
    "    print(f\"Braille Lookup Column: {'Generated Audio Response (Text for TTS)'}\")\n",
    "    print(\"-\" * 50)\n",
    "    speak(f\"I found the knowledge base with {len(KNOWLEDGE_BASE_DF)} entries.\")\n",
    "\n",
    "# --- 2. VOICE INPUT MODULE (LOCAL WHISPER) ---\n",
    "\n",
    "def listen():\n",
    "    \"\"\"Captures audio and converts it to text using local Whisper.\"\"\"\n",
    "    r = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"\\n\\nüéôÔ∏è Listening...\")\n",
    "        r.adjust_for_ambient_noise(source, duration=0.5)\n",
    "        try:\n",
    "            audio = r.listen(source, timeout=8, phrase_time_limit=15) \n",
    "        except sr.WaitTimeoutError:\n",
    "            print(\"‚ùå No speech detected within time limit.\")\n",
    "            return \"\"\n",
    "\n",
    "    try:\n",
    "        # --- LOCAL WHISPER STT (CPU PRIORITY) ---\n",
    "        if WHISPER_MODEL:\n",
    "            temp_wav_file = \"temp_audio_stt.wav\"\n",
    "            with open(temp_wav_file, \"wb\") as f:\n",
    "                f.write(audio.get_wav_data())\n",
    "\n",
    "            print(\"  [STT] Processing locally with Whisper (Tiny)...\")\n",
    "            result = WHISPER_MODEL.transcribe(\n",
    "                temp_wav_file, \n",
    "                initial_prompt=WHISPER_PROMPT, \n",
    "                language='en',\n",
    "                beam_size=3\n",
    "            )\n",
    "            text = result[\"text\"].strip()\n",
    "            os.remove(temp_wav_file)\n",
    "            \n",
    "        else:\n",
    "            # Fallback to Google STT if Whisper failed to load (requires internet)\n",
    "            text = r.recognize_google(audio)\n",
    "            \n",
    "        print(f\"üó£Ô∏è You said: {text}\")\n",
    "        return text\n",
    "        \n",
    "    except sr.UnknownValueError:\n",
    "        print(\"‚ùå Sorry, could not recognize speech. (Tip: Try speaking slower, or rephrase technical terms.)\") \n",
    "        return \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå STT Error: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# --- 3. TEXT-TO-SPEECH (TTS) MODULE (MODIFIED FOR PYTTSX3) ---\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Converts text to speech using pyttsx3 (offline).\"\"\"\n",
    "    global LAST_RESPONSE_TEXT\n",
    "    \n",
    "    if not text:\n",
    "        return\n",
    "        \n",
    "    LAST_RESPONSE_TEXT = text\n",
    "    \n",
    "    if TTS_ENGINE:\n",
    "        try:\n",
    "            print(f\"üí° AI Tutor: {text}\")\n",
    "            TTS_ENGINE.say(text)\n",
    "            TTS_ENGINE.runAndWait() \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during pyttsx3 playback: {e}\")\n",
    "    else:\n",
    "        print(f\"üí° AI Tutor: {text} (TTS Engine Failed)\")\n",
    "\n",
    "# --- 4. BRAILLE MODULES ---\n",
    "\n",
    "def braille_convert_by_last_answer(last_text_response):\n",
    "    \"\"\"\n",
    "    Finds the associated Braille output by matching the Query ID extracted from \n",
    "    the last response (e.g., '(E002)').\n",
    "    \"\"\"\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return speak(\"Knowledge base not loaded to find Braille content.\")\n",
    "    \n",
    "    # Extract Source ID from the start of the response text\n",
    "    if not last_text_response.startswith('(') or ')' not in last_text_response:\n",
    "        return speak(\"I cannot find a source ID in the last answer to look up Braille content. Please ask a new question first, or use the command 'Braille [Query ID]'.\")\n",
    "\n",
    "    source_id = last_text_response.split(')')[0].replace('(', '').strip()\n",
    "    \n",
    "    return braille_convert_by_id(source_id)\n",
    "\n",
    "def braille_convert_by_id(query_id):\n",
    "    \"\"\"\n",
    "    Finds the associated Braille output directly using a provided Query ID (e.g., 'E002').\n",
    "    \"\"\"\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return speak(\"Knowledge base not loaded to find Braille content.\")\n",
    "\n",
    "    # Standardize the ID for lookup\n",
    "    query_id = query_id.upper()\n",
    "    \n",
    "    # 1. Lookup the row using the reliable Query ID\n",
    "    match = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['Query ID'] == query_id\n",
    "    ]\n",
    "    \n",
    "    if match.empty:\n",
    "        return speak(f\"I cannot find the query ID {query_id} in the knowledge base.\")\n",
    "\n",
    "    # 2. Check if the row requires Braille output\n",
    "    if match['Braille Conversion Required (Yes/No)'].str.lower().str.contains('yes', na=False).any():\n",
    "        key_term = match.iloc[0]['Braille Output (Key Term/Formula/Code)']\n",
    "        \n",
    "        # --- Actual Braille Simulation ---\n",
    "        print(\"-\" * 50)\n",
    "        print(\"üñ®Ô∏è TACTILE OUTPUT INITIATED üñ®Ô∏è\")\n",
    "        print(f\"    Source ID: {query_id}\")\n",
    "        print(f\"    Key Term for Braille: {key_term}\")\n",
    "        print(\"-\" * 50)\n",
    "        speak(f\"The key information: {key_term} from source {query_id} has been sent to the Braille embosser.\")\n",
    "        \n",
    "    else:\n",
    "        speak(f\"The answer from source {query_id} does not require a Braille output.\")\n",
    "        \n",
    "# --- 5. RAG CORE MODULE ---\n",
    "\n",
    "def find_best_context(question):\n",
    "    \"\"\"\n",
    "    TARGETED RETRIEVAL: Uses keyword mapping to search the highest-confidence column.\n",
    "    \"\"\"\n",
    "    question_lower = question.lower()\n",
    "    \n",
    "    # 1. Check for direct keyword matches\n",
    "    for term, target_col in TECHNICAL_KEYWORDS.items():\n",
    "        if term in question_lower:\n",
    "            # Use regex=True for robust matching against Excel data\n",
    "            matching_rows = KNOWLEDGE_BASE_DF[\n",
    "                KNOWLEDGE_BASE_DF[target_col].astype(str).str.lower().str.contains(term, regex=True, na=False)\n",
    "            ]\n",
    "            \n",
    "            if not matching_rows.empty:\n",
    "                # SUCCESS: Use the best match (first one found)\n",
    "                row = matching_rows.iloc[0]\n",
    "                context = row['Context/Source (Pre-stored or AI-Gen)']\n",
    "                source_id = row['Query ID']\n",
    "                \n",
    "                print(f\"  [RAG STEP 1] Targeted Retrieval Success using '{term}'. Document ID: {source_id}\")\n",
    "                return context, source_id\n",
    "                \n",
    "    # 2. FALLBACK: Search the 'User Voice Input' column for the complete, transcribed query\n",
    "    print(\"  [RAG STEP 1] Fallback: No targeted keyword found. Searching full content.\")\n",
    "    \n",
    "    # Use a simpler match for fallback to prevent too many regex failures\n",
    "    matching_rows_fallback = KNOWLEDGE_BASE_DF[\n",
    "        KNOWLEDGE_BASE_DF['User Voice Input (Engineering Topic)'].astype(str).str.lower().str.contains(question_lower, na=False)\n",
    "    ]\n",
    "    \n",
    "    if not matching_rows_fallback.empty:\n",
    "        row = matching_rows_fallback.iloc[0]\n",
    "        context = row['Context/Source (Pre-stored or AI-Gen)']\n",
    "        source_id = row['Query ID']\n",
    "        \n",
    "        print(f\"  [RAG STEP 1] Fallback Success. Context found by matching full query: {source_id}\")\n",
    "        return context, source_id\n",
    "        \n",
    "    # 3. FAILURE: If RAG fails entirely.\n",
    "    return None, None \n",
    "\n",
    "def answer_question(question):\n",
    "    \"\"\"\n",
    "    RAG Pipeline: Retrieve -> Generate\n",
    "    \"\"\"\n",
    "    global QA_MODEL\n",
    "    global KNOWLEDGE_BASE_DF\n",
    "\n",
    "    if KNOWLEDGE_BASE_DF is None:\n",
    "        return \"Knowledge base not ready.\"\n",
    "\n",
    "    # --- 1. RETRIEVAL (RAG STEP) ---\n",
    "    context, source_id = find_best_context(question)\n",
    "    \n",
    "    if context is None:\n",
    "        return \"I apologize, that specific engineering topic is not yet in my knowledge base. Try asking about Ohm's Law or P-N junctions.\"\n",
    "        \n",
    "    # --- 2. GENERATION (QA MODEL) ---\n",
    "    try:\n",
    "        if QA_MODEL is None:\n",
    "            return \"QA model is not initialized.\"\n",
    "            \n",
    "        result = QA_MODEL({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "        \n",
    "        # RAG OBSERVABILITY: Print the source used for the answer\n",
    "        print(f\"  [RAG STEP 2] Answer generated from Source ID: {source_id}\")\n",
    "        \n",
    "        # Append the source to the final answer text for visual proof in the demo\n",
    "        final_answer = f\"({source_id}) {result['answer']}\"\n",
    "        return final_answer\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during QA processing: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not load_knowledge_base():  \n",
    "        print(\"\\nFATAL ERROR: Cannot proceed without the knowledge base. Check file name/path.\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(\"ü§ñ AI Tutor is ready. Speak your engineering question.\")\n",
    "        print(\"=\"*50)\n",
    "        speak(\"Hello! I am your AI engineering tutor. How can I help you learn today?\")\n",
    "        \n",
    "        while True:\n",
    "            query = listen()\n",
    "            \n",
    "            if not query:\n",
    "                continue\n",
    "                \n",
    "            query_lower = query.lower()\n",
    "            \n",
    "            # 3. Command Handling\n",
    "            \n",
    "            # 3a. EXIT/QUIT COMMAND\n",
    "            if query_lower in [\"exit\", \"quit\", \"stop\", \"i am done\"]:\n",
    "                speak(\"Goodbye! Happy studying.\")\n",
    "                print(\"üëã Session ended.\")\n",
    "                break\n",
    "            \n",
    "            # 3b. SHOW DATA COMMAND\n",
    "            elif \"show data\" in query_lower or \"show knowledge\" in query_lower:\n",
    "                show_data_status()\n",
    "                continue\n",
    "                \n",
    "            # 3c. REPLAY COMMAND\n",
    "            elif \"read again\" in query_lower or \"say that again\" in query_lower or \"replay\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    # Remove source ID from replay for cleaner audio\n",
    "                    clean_response = LAST_RESPONSE_TEXT.split(') ', 1)[-1]\n",
    "                    speak(f\"Replaying last response: {clean_response}\")\n",
    "                else:\n",
    "                    speak(\"I don't have a previous response to repeat.\")\n",
    "                continue\n",
    "\n",
    "            # 3d. DIRECT BRAILLE COMMAND (New) - Braille E002, Braille E005, etc.\n",
    "            match_id = re.search(r'braille\\s+([e]\\d+)', query_lower)\n",
    "            if match_id:\n",
    "                query_id = match_id.group(1).upper()\n",
    "                braille_convert_by_id(query_id)\n",
    "                continue\n",
    "                \n",
    "            # 3e. BRAILLE LAST ANSWER COMMAND (Original)\n",
    "            elif \"braille\" in query_lower or \"tactile\" in query_lower or \"print\" in query_lower:\n",
    "                if LAST_RESPONSE_TEXT:\n",
    "                    braille_convert_by_last_answer(LAST_RESPONSE_TEXT)\n",
    "                else:\n",
    "                    speak(\"I need to answer a question before I can convert a response to Braille. Try asking a question or using the command 'Braille [Query ID]'.\")\n",
    "                continue\n",
    "\n",
    "            # 4. Question Answering (RAG Pipeline)\n",
    "            response = answer_question(query)\n",
    "            speak(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b112ab6-c945-45c0-8fbc-9d5bb1e5d4d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai_tutor_env)",
   "language": "python",
   "name": "ai_tutor_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
